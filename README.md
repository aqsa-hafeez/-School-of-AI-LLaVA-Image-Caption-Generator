# Image Caption Generator (LLaVA + Ollama + FastAPI + Streamlit)

A full-stack AI application that generates natural-language captions for uploaded images using a locally hosted LLaVA vision-language model via Ollama.

This project demonstrates how to integrate a vision-language model into a modern backend-frontend architecture.

---

## ğŸš€ Project Overview

This application allows users to upload an image and receive a descriptive caption generated by a vision-language AI model.

The system architecture:

User â†’ Streamlit (Frontend) â†’ FastAPI (Backend) â†’ Ollama â†’ LLaVA Model

---

## ğŸ› ï¸ Tech Stack

- **Ollama** â€“ Local model hosting  
- **LLaVA** â€“ Vision-Language model for image captioning  
- **FastAPI** â€“ Backend API service  
- **Streamlit** â€“ Frontend user interface  
- **Pillow** â€“ Image processing in Python  
- **Python** â€“ Core programming language  
- **Git & GitHub** â€“ Version control  

---

## ğŸ“‚ Project Structure

```

image-caption-llava/
â”‚
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ frontend/
â”‚   â””â”€â”€ app.py
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

````

---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/aqsa-hafeez/image-caption-llava.git
cd image-caption-llava
````

---

### 2ï¸âƒ£ Create Virtual Environment

```bash
python -m venv venv
```

Activate:

**Windows**

```bash
venv\Scripts\activate
```

**Mac/Linux**

```bash
source venv/bin/activate
```

---

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

---

### 4ï¸âƒ£ Install Ollama & Pull LLaVA Model

Download Ollama:
ğŸ‘‰ [https://ollama.com](https://ollama.com)

Pull the model:

```bash
ollama pull llava
```

Verify installation:

```bash
ollama list
```

---

## â–¶ï¸ Running the Application

### Step 1: Start Backend

```bash
uvicorn backend.main:app --reload
```

Backend will run at:

```
http://127.0.0.1:8000
```

âš ï¸ Keep this terminal open.

---

### Step 2: Start Frontend (New Terminal)

```bash
streamlit run frontend/app.py
```

Frontend will open in browser at:

```
http://localhost:8501
```

---

## ğŸ“Œ How It Works

1. User uploads an image in Streamlit UI
2. Streamlit sends the image to FastAPI backend
3. Backend encodes the image to base64 and sends it to Ollama
4. LLaVA model generates a descriptive caption
5. Caption is returned and displayed in Streamlit

---
